# ðŸŽµ AI Music Generation â€” The Complete Ready Reckoner

> **Class companion guide** based on the presentation *"AI Music Generation and the Tech Behind It"*
>
> ðŸ“Ž [View the original presentation on Canva](https://www.canva.com/design/DAHAp6P8GUk/cT8mRkeHErfI_qnzmKmqJw/edit)

---

## ðŸ“‹ Table of Contents

1. [The Big Picture](#1--the-big-picture)
2. [A Brief History â€” From 1936 to Today](#2--a-brief-history--from-1936-to-today)
3. [The 2010s Revolution â€” Deep Learning Enters Music](#3--the-2010s-revolution--deep-learning-enters-music)
4. [How Sound Works â€” Waveforms & Spectrograms](#4--how-sound-works--waveforms--spectrograms)
5. [The AI Models Behind Music Generation](#5--the-ai-models-behind-music-generation)
6. [Key Architectures Explained](#6--key-architectures-explained)
7. [Real-World Applications](#7--real-world-applications)
8. [The Other Side of the Coin â€” Ethics & Controversy](#8--the-other-side-of-the-coin--ethics--controversy)
9. [AI Music Generation Tools You Can Try Today](#9--ai-music-generation-tools-you-can-try-today)
10. [Interactive Experiments & Demos](#10--interactive-experiments--demos)
11. [Resources for Further Learning](#11--resources-for-further-learning)
12. [Quick Revision Cheat Sheet](#12--quick-revision-cheat-sheet)

---

## 1. ðŸŽ¯ The Big Picture

**Can you tell the difference between a song written by a human and one generated by AI?**

That was the opening question of our class â€” and it's becoming harder to answer every day. AI music generation has evolved from crude beeping sounds in the 1930s to producing full studio-quality tracks that fool professional musicians.

This guide walks you through **the history, the technology, the models, the tools, and the ethics** of AI-generated music.

### Why Should You Care?

- The global AI music market is growing rapidly, impacting every part of the music industry
- Understanding how these models work gives you an edge whether you're in tech, business, or creative fields
- The ethical and legal debates around AI music will shape policy and business for decades

---

## 2. ðŸ“œ A Brief History â€” From 1936 to Today

The story of AI and music is much older than most people think. Here's the timeline:

### ðŸ•°ï¸ 1936 â€” Where It All Started

Alan Turing's foundational work on computation laid the groundwork for everything that followed. The concept of a "universal machine" that could simulate any computation was the seed that would eventually grow into music-generating AI.

### ðŸ–¥ï¸ 1951 â€” The First Computer Music

The earliest known recording of computer-generated music came from a machine built by Alan Turing at the University of Manchester. The computer played three melodies: "God Save the King," "Baa Baa Black Sheep," and "In the Mood" by Glenn Miller. This wasn't AI composing music â€” it was a computer being programmed to play notes â€” but it proved that machines could produce musical sound.

> ðŸ”— **Listen to it yourself:** [First Recording of Computer Music (Open Culture)](https://www.openculture.com/2016/09/hear-the-first-recording-of-computer-music.html)
>
> ðŸŽ§ **YouTube Recording:** [1951 Computer Music](https://www.youtube.com/watch?v=ZZ0BOEOtD2U)

### ðŸ“¡ 1950sâ€“1960s â€” Early Experiments

Researchers at Bell Labs and universities began experimenting with algorithmic composition. The **Illiac Suite** (1957) by Lejaren Hiller and Leonard Isaacson was the first score composed by a computer, using Markov chains and rule-based systems.

> ðŸ”— **Further reading:** [History of Information â€” Computer Music](https://www.historyofinformation.com/detail.php?id=3886)

### ðŸŽ¹ 1970s â€” Synthesizers Meet Algorithms

The 1970s saw the rise of electronic synthesizers and early software tools that could assist in music composition. Composers began exploring the intersection of electronic sound and computational logic. Tools like **Music Mouse** (by Laurie Spiegel) turned simple gestures into complex musical compositions, making algorithmic music accessible to non-programmers.

> ðŸ”— **Try Music Mouse:** [teropa.info/musicmouse](https://teropa.info/musicmouse/)

### ðŸ’¾ 1980sâ€“2000s â€” MIDI, Digital Audio, and Rule-Based Systems

The introduction of MIDI (Musical Instrument Digital Interface) in 1983 standardized how electronic instruments communicate. This era saw rule-based AI systems that could compose simple melodies using music theory rules, Markov chains, and genetic algorithms. The results were interesting but lacked the expressiveness and coherence of human composition.

### ðŸš€ 2010s â€” The Deep Learning Explosion

Everything changed. Neural networks â€” particularly deep learning â€” gave AI the ability to learn patterns from massive datasets of music rather than following hand-coded rules. This is where the story gets really exciting...

---

## 3. ðŸ§  The 2010s Revolution â€” Deep Learning Enters Music

By the 2010s, three groups of people converged to create the AI music revolution:

### The Three Pillars

| Who | What They Brought |
|-----|-------------------|
| **Musicians** | Creative vision, understanding of what "good music" sounds like, desire for new tools |
| **Researchers** | Deep learning expertise, neural network architectures, published papers |
| **Deep Learning (the tech)** | CNNs, RNNs, Transformers, GANs â€” models that could learn from raw audio data |

### Two Landmark Projects

#### ðŸŽ¨ Google Magenta (2016)

**What is it?** An open-source research project by the Google Brain team exploring how machine learning can be used to create art and music.

**Why does it matter?**
- Made AI music generation **accessible and open-source**
- Built on TensorFlow, providing tools anyone could use
- Created models like **MelodyRNN**, **MusicVAE**, and **NSynth** (Neural Synthesizer)
- Proved that neural networks could learn musical structure (rhythm, harmony, melody)
- Released **Magenta Studio** â€” standalone tools and Ableton Live plugins for producers
- In 2025, released **Magenta RealTime (Magenta RT)** â€” an 800M parameter open-weights model for live, interactive music generation

**Key Models in Magenta:**

| Model | What It Does |
|-------|-------------|
| **MelodyRNN** | Generates melodies one note at a time using recurrent neural networks |
| **MusicVAE** | Uses Variational Autoencoders to blend and interpolate between musical ideas |
| **NSynth** | Creates entirely new sounds by learning from a dataset of instrument sounds |
| **Music Transformer** | Uses self-attention to capture long-range musical structure |
| **Magenta RT** | Real-time music generation steered by text or audio prompts |

> ðŸ”— **Magenta Homepage:** [magenta.withgoogle.com](https://magenta.withgoogle.com/)
>
> ðŸ”— **GitHub:** [github.com/magenta/magenta](https://github.com/magenta/magenta)
>
> ðŸ”— **Listen to Transformer (100k samples):** [magenta.github.io/listen-to-transformer](https://magenta.github.io/listen-to-transformer/)

---

#### ðŸŒŠ DeepMind's WaveNet (2016)

**What is it?** A deep generative model that works directly on **raw audio waveforms** â€” predicting one audio sample at a time.

**Why was it revolutionary?**
- Previous systems used vocoders and signal processing to generate speech. WaveNet modeled the raw waveform directly
- Produced speech that was **50% closer to human-level** quality compared to previous best systems
- Could generate music, not just speech â€” producing novel piano pieces
- Generated audio at **16,000+ samples per second**, capturing natural elements like breathing and lip movements
- A single WaveNet could learn multiple voices and switch between them

**How WaveNet Works (Simplified):**

```
Input: Previous audio samples â†’ [Dilated Causal Convolutions] â†’ Predicted next sample
                                           â†‘
                              (Stacked layers with increasing dilation)
                              (Captures patterns at multiple time scales)
```

1. **Autoregressive**: Each audio sample is predicted based on ALL previous samples
2. **Dilated Convolutions**: Special layers that "skip" over inputs, allowing the model to have a very large receptive field (it can "hear" context from far back) without requiring an enormous number of layers
3. **Î¼-law Companding**: Compresses 16-bit audio (65,536 values) to 8-bit (256 values) for tractable prediction
4. **Conditional Generation**: Can be conditioned on speaker identity, text, or other signals

**Impact:**
- Became the voice behind **Google Assistant** (US English & Japanese)
- Powered Google Maps Navigation, Voice Search, and Cloud Text-to-Speech
- Inspired WaveRNN (a faster, lighter variant for on-device deployment)
- Opened the door to high-quality AI voice cloning

> ðŸ”— **DeepMind Blog Post:** [deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/)
>
> ðŸ”— **Research Paper (arXiv):** [arxiv.org/abs/1609.03499](https://arxiv.org/abs/1609.03499)
>
> ðŸ”— **TensorFlow Implementation:** [github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet)

---

## 4. ðŸ”Š How Sound Works â€” Waveforms & Spectrograms

Before understanding AI music models, you need to understand how sound is represented digitally. This is fundamental.

### Waveform

A **waveform** is the most basic representation of audio â€” it shows how air pressure (amplitude) changes over time.

```
Amplitude
    â†‘
    |    /\      /\      /\
    |   /  \    /  \    /  \
    |--/----\--/----\--/----\---â†’ Time
    |        \/      \/      \/
    |
```

**Key properties:**
- **Amplitude** = How loud the sound is (height of the wave)
- **Frequency** = How many cycles per second (pitch) â€” measured in Hertz (Hz)
- **Sample Rate** = How many times per second the audio is measured (e.g., 44,100 Hz for CD quality, 16,000 Hz for speech)

> ðŸ”— **Interactive Demo:** [Chrome Music Lab â€” Sound Waves](https://musiclab.chromeexperiments.com/Sound-Waves/)

### Spectrogram

A **spectrogram** is a visual representation that shows **frequency content over time**. It's like an X-ray of sound.

```
Frequency (Hz)
    â†‘
    |  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ     â† High frequencies
    |  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â† Mid frequencies
    |  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â† Low frequencies
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Time
    
    (Brightness/Color = Intensity/Loudness)
```

**Why spectrograms matter for AI:**
- Many AI models work on spectrograms rather than raw waveforms because they compress the information efficiently
- A spectrogram is essentially a 2D image â€” so image-based neural networks (CNNs) can be applied to audio!
- **Mel spectrograms** use a scale that matches how humans perceive pitch (we hear differences in low frequencies more than high)

> ðŸ”— **Interactive Demo:** [Chrome Music Lab â€” Spectrogram](https://musiclab.chromeexperiments.com/spectrogram/)
>
> ðŸ’¡ **Try it yourself!** Open the spectrogram link, make sounds into your microphone, and watch how different sounds create different visual patterns.

---

## 5. ðŸ¤– The AI Models Behind Music Generation

Once you train neural networks on audio data, what do you get? Here's the landscape of machine learning models used for audio:

### Overview of ML Models for Audio

```
                    ML Models for Audio
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚             â”‚             â”‚
      Autoregressive   Diffusion     GANs
       Models          Models    (Generative
            â”‚             â”‚      Adversarial
            â”‚             â”‚       Networks)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”     â”‚         â”‚
    â”‚               â”‚     â”‚         â”‚
  WaveNet      Transformers â”‚     WaveGAN
  WaveRNN     (Music       â”‚     MelGAN
              Transformer)  â”‚
                            â”‚
                      AudioLDM
                      Stable Audio
```

### The Main Families

#### 1. Autoregressive Models
**How they work:** Generate audio one sample (or token) at a time, where each new sample depends on all previous ones.

**Think of it like:** Writing a story one word at a time, where each word depends on everything you've written before.

**Examples:** WaveNet, WaveRNN, SampleRNN, Music Transformer

**Pros:** High quality, captures fine details
**Cons:** Slow generation (sequential by nature)

#### 2. Generative Adversarial Networks (GANs)
**How they work:** Two networks compete â€” a **Generator** creates fake audio, and a **Discriminator** tries to tell if it's real or fake. They push each other to improve.

**Think of it like:** A counterfeiter (Generator) and a detective (Discriminator) in an arms race. The counterfeiter keeps making better fakes; the detective keeps getting better at spotting them.

**Examples:** WaveGAN, MelGAN, GAN-TTS

**Pros:** Fast generation, good at creating realistic textures
**Cons:** Training can be unstable, may lack musical coherence

#### 3. Diffusion Models
**How they work:** Start with pure noise and gradually "denoise" it into audio, guided by learned patterns from training data.

**Think of it like:** Starting with a completely blurry/noisy photo and gradually sharpening it into a clear picture.

**Examples:** AudioLDM, Stable Audio, Riffusion, Noise2Music

**Pros:** Very high quality, stable training
**Cons:** Many denoising steps needed (can be slow)

#### 4. Transformer-Based Models
**How they work:** Use the **self-attention mechanism** to capture long-range dependencies in sequences. Each element can "attend to" any other element regardless of distance.

**Think of it like:** Instead of reading a book word by word, you can instantly jump to any relevant passage to understand context.

**Examples:** Music Transformer, MusicLM, Jukebox, MusicGen, Suno (uses transformers internally)

**Pros:** Captures long-range structure (verse-chorus-verse), handles complex patterns
**Cons:** Computationally expensive, needs large datasets

> ðŸ”— **GenAU â€” Generative Audio Understanding:** [snap-research.github.io/GenAU](https://snap-research.github.io/GenAU/)

---

## 6. ðŸ—ï¸ Key Architectures Explained

### Self-Attention Mechanism (Transformers)

The **self-attention mechanism** is the core innovation behind Transformers, and it's what makes modern music AI so powerful.

**The Problem:** In a long piece of music, a note played in the chorus should relate to the same motif played in the intro (maybe 30 seconds earlier). Older models (RNNs) struggled with these long-range dependencies because information "faded" as sequences got longer.

**The Solution â€” Self-Attention:**

```
Every element in the sequence can directly "attend to" every other element.

    Note 1  â†â†’  Note 2  â†â†’  Note 3  â†â†’  ...  â†â†’  Note N
      â†•           â†•           â†•                      â†•
    (Each note computes a weighted relationship with ALL other notes)
```

**How it works in 4 steps:**

1. **Query, Key, Value**: Each input element is transformed into three vectors â€” a Query (what am I looking for?), a Key (what do I contain?), and a Value (what information do I carry?)
2. **Attention Scores**: The Query of each element is compared with the Keys of all other elements to compute similarity scores
3. **Softmax**: Scores are normalized so they sum to 1 (like a probability distribution)
4. **Weighted Sum**: Each element's output is a weighted combination of all Values, weighted by the attention scores

**For music, this means:** A note being generated can "look back" at the entire piece so far and decide which earlier notes are most relevant â€” enabling coherent musical structure over long durations.

### ARDMs (Autoregressive Diffusion Models)

ARDMs combine the best of both worlds â€” the sequential, high-quality generation of autoregressive models with the noise-to-signal approach of diffusion models. They generate data by progressively "unmasking" or "denoising" tokens in a flexible order.

### Variational Autoencoders (VAEs)

Used in models like **MusicVAE** (from Magenta):

```
Input Audio â†’ [Encoder] â†’ Latent Space (compressed representation) â†’ [Decoder] â†’ Reconstructed Audio
                              â†‘
                    (This is where the "creativity" happens â€”
                     you can sample new points in latent space
                     to generate new music)
```

**Why VAEs are cool for music:** You can take two different melodies, find their positions in latent space, and smoothly interpolate between them to create a blend that transitions naturally from one to the other.

---

## 7. ðŸŒ Real-World Applications

AI audio generation isn't just a research curiosity â€” it's being used across many industries:

### ðŸ—£ï¸ AI Voice Assistants
Google Assistant, Siri, and Alexa all use neural TTS (Text-to-Speech) models descended from WaveNet and similar architectures. These voices sound increasingly natural, with proper intonation, emotion, and pacing.

### ðŸŽ¼ Music Composition & Production
AI tools can now compose full tracks, generate backing music, create sound effects, and even master audio. Producers use AI for inspiration, to break creative blocks, or to quickly prototype ideas.

### ðŸŽ® Video Game Audio
Games can use AI to generate **adaptive soundtracks** that respond to player actions in real-time. Instead of looping pre-recorded tracks, the game generates unique music that matches the emotional tone of what's happening.

### ðŸŽ¤ AI Voice Cloning
Models can learn the characteristics of a specific voice from a small number of recordings and then generate new speech in that voice saying anything. This has both exciting and concerning implications.

**The experiment from class:**
- We heard an **original voice recording** and then an **AI-generated clone** of that same voice
- The quality was striking â€” the AI voice captured the tone, accent, and speaking style of the original speaker

### ðŸŽ¬ Film, Podcasts & Content Creation
AI-generated music and voiceovers are increasingly used in content production, especially where budget or time constraints make hiring musicians or voice actors impractical.

### ðŸ§ª Research & Education
Tools like **NotebookLM** by Google use AI to generate audio summaries and podcast-style content from documents.

> ðŸ”— **NotebookLM:** [notebooklm.google.com](https://notebooklm.google.com)

---

## 8. âš–ï¸ The Other Side of the Coin â€” Ethics & Controversy

Every powerful technology comes with ethical challenges. AI music generation is no exception.

### Key Concerns

**ðŸ”’ Copyright & Ownership**
- Who owns a song generated by AI? The person who typed the prompt? The company that built the model? The artists whose music was in the training data?
- Multiple lawsuits are ongoing between AI music companies and record labels/artists

**ðŸŽ­ Deepfakes & Unauthorized Voice Cloning**
- The "AI Eminem" track used at a David Guetta concert went viral â€” Eminem's voice was cloned without permission
- Artists like Drake, The Weeknd, and others have had their voices cloned by fans and pranksters
- This raises serious questions about consent and identity

**ðŸ’° Economic Impact on Musicians**
- If AI can generate a "good enough" background track in seconds, what happens to session musicians, jingle composers, and soundtrack artists?
- Counter-argument: AI tools can also *democratize* music creation, letting people who can't play instruments still express musical ideas

**ðŸ“Š Training Data Ethics**
- Most AI music models are trained on copyrighted music without explicit permission
- The legal framework is still being established globally

**ðŸ¤” Authenticity & Art**
- Is AI-generated music "real" music? Does it have artistic value?
- Can AI capture the emotional depth and intentionality that makes music meaningful to humans?

### The Viral Moments

- **"Heart on My Sleeve"** â€” An AI-generated track mimicking Drake and The Weeknd went viral in 2023, sparking massive debate
- **David Guetta's "AI Eminem"** â€” Guetta used AI to generate a verse in Eminem's style during a live set, raising questions about live performance and attribution
- **The Grammy Debate** â€” Can AI-generated music be nominated for awards? The Recording Academy has been grappling with new rules

---

## 9. ðŸ› ï¸ AI Music Generation Tools You Can Try Today

Here are the major platforms available right now:

### ðŸŽµ [Suno](https://suno.ai)
- **What:** Full song generation from text prompts (lyrics + music)
- **Best for:** Complete song creation with vocals, instruments, and production
- **Highlight:** Can generate genre-specific songs with custom lyrics in minutes
- **Free tier:** Yes, with limited generations per day

### ðŸŽµ [Udio](https://udio.com)
- **What:** AI music generation with focus on high-quality, diverse outputs
- **Best for:** Exploring different genres and styles with nuanced control
- **Highlight:** Strong at capturing genre-specific production styles

### ðŸŽµ [ElevenLabs](https://elevenlabs.io)
- **What:** AI voice synthesis and cloning platform
- **Best for:** Text-to-speech, voice cloning, audiobook narration, dubbing
- **Highlight:** Ultra-realistic voice cloning from short audio samples, supports 29+ languages

### ðŸŽµ [Musicfy](https://musicfy.lol)
- **What:** AI music generation with voice conversion capabilities
- **Best for:** Converting your voice to sound like different instruments or styles
- **Highlight:** Real-time voice-to-instrument conversion

### ðŸŽ¨ [Google Magenta Studio](https://magenta.withgoogle.com/)
- **What:** Open-source AI music tools (plugins for Ableton Live and standalone apps)
- **Best for:** MIDI-based music generation, experimentation, learning
- **Highlight:** Free, open-source, integrates with professional DAWs

### ðŸŽµ [AIVA](https://www.aiva.ai)
- **What:** AI composer for film, game, and commercial music
- **Best for:** Soundtrack and score composition
- **Highlight:** Trained on classical music; recognized as a composer by SACEM (French professional association of authors and composers)

---

## 10. ðŸ§ª Interactive Experiments & Demos

One of the best ways to learn is by doing. Here are hands-on demos you can try right now:

| Demo | What You'll Learn | Link |
|------|-------------------|------|
| **Chrome Music Lab â€” Sound Waves** | How sound waves work visually | [Try it â†’](https://musiclab.chromeexperiments.com/Sound-Waves/) |
| **Chrome Music Lab â€” Spectrogram** | How different sounds look as spectrograms | [Try it â†’](https://musiclab.chromeexperiments.com/spectrogram/) |
| **Music Mouse** | Algorithmic composition from the 1970s | [Try it â†’](https://teropa.info/musicmouse/) |
| **Listen to Transformer** | 100,000 AI-generated piano pieces (Magenta) | [Try it â†’](https://magenta.github.io/listen-to-transformer/) |
| **NotebookLM** | AI-generated audio summaries from documents | [Try it â†’](https://notebooklm.google.com) |
| **Suno** | Generate a full song from a text prompt | [Try it â†’](https://suno.ai) |

---

## 11. ðŸ“š Resources for Further Learning

### ðŸ”¬ Research Papers & Technical Resources

| Resource | Description | Link |
|----------|-------------|------|
| **WaveNet Paper** | The original DeepMind paper on raw audio generation | [arXiv](https://arxiv.org/abs/1609.03499) |
| **WaveNet Blog** | DeepMind's accessible blog post explaining WaveNet | [DeepMind](https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/) |
| **GenAU** | Snap Research â€” Generative Audio Understanding | [Website](https://snap-research.github.io/GenAU/) |
| **Magenta GitHub** | Source code for all Magenta models and tools | [GitHub](https://github.com/magenta/magenta) |
| **Magenta RT (HuggingFace)** | Open-weights real-time music model | [HuggingFace](https://huggingface.co/google/magenta-realtime) |

### ðŸ“– Articles & History

| Resource | Description | Link |
|----------|-------------|------|
| **First Computer Music** | Hear the 1951 recording of computer-generated music | [Open Culture](https://www.openculture.com/2016/09/hear-the-first-recording-of-computer-music.html) |
| **History of Computer Music** | Detailed history of information on early computer music | [History of Information](https://www.historyofinformation.com/detail.php?id=3886) |
| **1951 Computer Music (Video)** | YouTube recording of the earliest computer music | [YouTube](https://www.youtube.com/watch?v=ZZ0BOEOtD2U) |

### ðŸ’» Practice & Hands-On

| Resource | Description | Link |
|----------|-------------|------|
| **Kaggle** | Datasets and notebooks for audio/music ML | [kaggle.com](https://www.kaggle.com) |
| **TensorFlow WaveNet** | Open-source WaveNet implementation | [GitHub](https://github.com/ibab/tensorflow-wavenet) |
| **Chrome Music Lab** | Interactive experiments to understand sound | [musiclab.chromeexperiments.com](https://musiclab.chromeexperiments.com/) |
| **Music Mouse** | Try 1970s-era algorithmic music composition | [teropa.info/musicmouse](https://teropa.info/musicmouse/) |

---

## 12. ðŸ“ Quick Revision Cheat Sheet

### Timeline at a Glance

```
1936  â†’  Turing's foundational computation work
1951  â†’  First computer music (University of Manchester)
1957  â†’  Illiac Suite â€” first computer-composed score
1970s â†’  Synthesizers + algorithmic tools (Music Mouse)
1983  â†’  MIDI standard introduced
2014  â†’  GANs invented (Goodfellow et al.)
2016  â†’  Google Magenta launched
2016  â†’  DeepMind releases WaveNet
2017  â†’  Transformers paper ("Attention Is All You Need")
2018  â†’  Music Transformer by Magenta
2019  â†’  Magenta Studio released (Ableton plugins)
2020  â†’  Jukebox by OpenAI
2023  â†’  Suno, Udio emerge as consumer AI music tools
2023  â†’  "Heart on My Sleeve" AI Drake/Weeknd goes viral
2024  â†’  MusicLM, Stable Audio, and more
2025  â†’  Magenta RealTime (open-weights live model)
```

### Key Concepts â€” One-Liner Definitions

| Concept | Definition |
|---------|------------|
| **Waveform** | A graph of air pressure (amplitude) over time â€” the raw representation of sound |
| **Spectrogram** | A visual showing frequency content over time â€” like a heatmap of sound |
| **Mel Spectrogram** | A spectrogram using a scale that matches human hearing perception |
| **Autoregressive Model** | Generates output one step at a time, each step conditioned on all previous steps |
| **GAN** | Two competing networks (Generator vs Discriminator) that improve each other |
| **Diffusion Model** | Generates data by gradually denoising from random noise |
| **Transformer** | Architecture using self-attention to capture relationships across entire sequences |
| **Self-Attention** | Mechanism allowing every element to directly relate to every other element |
| **VAE** | Encoder-decoder model with a smooth latent space for interpolation and generation |
| **WaveNet** | DeepMind's autoregressive model that generates raw audio waveforms sample by sample |
| **Magenta** | Google's open-source project for AI-powered music and art creation |
| **MIDI** | A protocol for electronic instruments to communicate (notes, not audio) |
| **Vocoder** | A signal processing tool that synthesizes speech/audio from parameters |
| **Î¼-law Companding** | Compression technique that converts 16-bit audio to 8-bit for easier processing |
| **Latent Space** | A compressed, learned representation where similar inputs are close together |

### Model Comparison Table

| Model | Type | Input | Output | Speed | Quality | Best For |
|-------|------|-------|--------|-------|---------|----------|
| **WaveNet** | Autoregressive CNN | Previous samples | Raw audio | Slow | Excellent | Speech, piano music |
| **Music Transformer** | Autoregressive Transformer | MIDI tokens | MIDI sequences | Medium | Very Good | Piano, structured music |
| **MusicVAE** | VAE | MIDI | MIDI | Fast | Good | Interpolation, blending |
| **WaveGAN** | GAN | Noise vector | Raw audio | Fast | Good | Sound effects, short clips |
| **AudioLDM** | Diffusion | Text prompt | Audio | Medium | Excellent | Text-to-audio |
| **Suno/Udio** | Transformer + Diffusion | Text prompt | Full songs | Fast | Excellent | Complete song generation |

---

## ðŸŽ“ Final Thought

> *"AI doesn't replace musicians â€” it gives everyone a new instrument to play."*

The technology behind AI music generation draws from decades of research in signal processing, neural networks, and creative AI. Whether you become a builder of these systems, a user of these tools, or a policymaker shaping their governance â€” understanding the fundamentals gives you a seat at the table.

**Go experiment. Break things. Make noise.** ðŸŽ¶

---

*This guide was created as a companion to the class presentation. Last updated: February 2026.*

*For questions or clarifications, reach out to your instructor.*
